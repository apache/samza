<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- Generated by Apache Maven Doxia at 2015-09-16 -->
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Apache Hadoop 2.6.1 - </title>
    <style type="text/css" media="all">
      @import url("./css/maven-base.css");
      @import url("./css/maven-theme.css");
      @import url("./css/site.css");
    </style>
    <link rel="stylesheet" href="./css/print.css" type="text/css" media="print" />
        <meta name="Date-Revision-yyyymmdd" content="20150916" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      </head>
  <body class="composite">
    <div id="banner">
                  <a href="http://hadoop.apache.org/" id="bannerLeft">
                                        <img src="http://hadoop.apache.org/images/hadoop-logo.jpg" alt="" />
                </a>
                        <a href="http://www.apache.org/" id="bannerRight">
                                        <img src="http://www.apache.org/images/asf_logo_wide.png" alt="" />
                </a>
            <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="breadcrumbs">
            
                                <div class="xleft">
                          <a href="http://www.apache.org/" class="externalLink">Apache</a>
                &gt;
                      <a href="http://hadoop.apache.org/" class="externalLink">Hadoop</a>
                &gt;
                      <a href="../">hadoop-mapreduce-client</a>
                &gt;
                Apache Hadoop 2.6.1
                </div>
            <div class="xright">            <a href="http://wiki.apache.org/hadoop" class="externalLink">Wiki</a>
            |
                <a href="https://svn.apache.org/repos/asf/hadoop/" class="externalLink">SVN</a>
            |
                <a href="http://hadoop.apache.org/" class="externalLink">Apache Hadoop</a>
              
                                &nbsp;| Last Published: 2015-09-16
              &nbsp;| Version: 2.6.1
            </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="leftColumn">
      <div id="navcolumn">
             
                                                <h5>General</h5>
                  <ul>
                  <li class="none">
                  <a href="../../index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/SingleCluster.html">Single Node Setup</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/ClusterSetup.html">Cluster Setup</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/CommandsManual.html">Hadoop Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/FileSystemShell.html">FileSystem Shell</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/Compatibility.html">Hadoop Compatibility</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/filesystem/index.html">FileSystem Specification</a>
            </li>
          </ul>
                       <h5>Common</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/CLIMiniCluster.html">CLI Mini Cluster</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/NativeLibraries.html">Native Libraries</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/Superusers.html">Superusers</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/SecureMode.html">Secure Mode</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/ServiceLevelAuth.html">Service Level Authorization</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/HttpAuthentication.html">HTTP Authentication</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-kms/index.html">Hadoop KMS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/Tracing.html">Tracing</a>
            </li>
          </ul>
                       <h5>HDFS</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html">HDFS User Guide</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HDFSCommands.html">HDFS Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">High Availability With QJM</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">High Availability With NFS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/Federation.html">Federation</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/ViewFs.html">ViewFs Guide</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">HDFS Snapshots</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsDesign.html">HDFS Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsEditsViewer.html">Edits Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsImageViewer.html">Image Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">Permissions and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html">Quotas and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/Hftp.html">HFTP</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/LibHdfs.html">C API libhdfs</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/WebHDFS.html">WebHDFS REST API</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-hdfs-httpfs/index.html">HttpFS Gateway</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html">Short Circuit Local Reads</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html">Centralized Cache Management</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html">HDFS NFS Gateway</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html">HDFS Rolling Upgrade</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/ExtendedAttributes.html">Extended Attributes</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html">Transparent Encryption</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/HdfsMultihoming.html">HDFS Support for Multihoming</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html">Archival Storage, SSD & Memory</a>
            </li>
          </ul>
                       <h5>MapReduce</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">MapReduce Tutorial</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredCommands.html">MapReduce Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html">Compatibilty between Hadoop 1.x and Hadoop 2.x</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/EncryptedShuffle.html">Encrypted Shuffle</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/PluggableShuffleAndPluggableSort.html">Pluggable Shuffle/Sort</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistributedCacheDeploy.html">Distributed Cache Deploy</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/HadoopStreaming.html">Hadoop Streaming</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/HadoopArchives.html">Hadoop Archives</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistCp.html">DistCp</a>
            </li>
          </ul>
                       <h5>MapReduce REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredAppMasterRest.html">MR Application Master</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html">MR History Server</a>
            </li>
          </ul>
                       <h5>YARN</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/YARN.html">YARN Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">Capacity Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/FairScheduler.html">Fair Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html">ResourceManager Restart</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">ResourceManager HA</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/WebApplicationProxy.html">Web Application Proxy</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html">YARN Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html">Writing YARN Applications</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/YarnCommands.html">YARN Commands</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-sls/SchedulerLoadSimulator.html">Scheduler Load Simulator</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/NodeManagerRestart.html">NodeManager Restart</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/DockerContainerExecutor.html">DockerContainerExecutor</a>
            </li>
          </ul>
                       <h5>YARN REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html">Introduction</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">Resource Manager</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-site/NodeManagerRest.html">Node Manager</a>
            </li>
          </ul>
                       <h5>Auth</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-auth/index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-auth/Examples.html">Examples</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-auth/Configuration.html">Configuration</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-auth/BuildingIt.html">Building</a>
            </li>
          </ul>
                       <h5>Reference</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/releasenotes.html">Release Notes</a>
            </li>
                  <li class="none">
                  <a href="../../api/index.html">API docs</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/CHANGES.txt">Common CHANGES.txt</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/CHANGES.txt">HDFS CHANGES.txt</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-mapreduce/CHANGES.txt">MapReduce CHANGES.txt</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-yarn/CHANGES.txt">YARN CHANGES.txt</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/Metrics.html">Metrics</a>
            </li>
          </ul>
                       <h5>Configuration</h5>
                  <ul>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/core-default.xml">core-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">hdfs-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">mapred-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../hadoop-project-dist/hadoop-common/DeprecatedProperties.html">Deprecated Properties</a>
            </li>
          </ul>
                                 <a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy">
          <img alt="Built by Maven" src="./images/logos/maven-feather.png"/>
        </a>
                       
                            </div>
    </div>
    <div id="bodyColumn">
      <div id="contentBox">
        <!-- -
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file. --><h1>DistCp Version2 Guide</h1>
<hr />

<ul>
  
<li><a href="#Overview">Overview</a></li>
  
<li><a href="#Usage">Usage</a>
  
<ul>
    
<li><a href="#Basic_Usage">Basic Usage</a></li>
    
<li><a href="#Update_and_Overwrite">Update and Overwrite</a></li>
  </ul></li>
  
<li><a href="#Command_Line_Options">Command Line Options</a></li>
  
<li><a href="#Architecture_of_DistCp">Architecture of DistCp</a>
  
<ul>
    
<li><a href="#DistCp_Driver">DistCp Driver</a></li>
    
<li><a href="#Copy-listing_Generator">Copy-listing Generator</a></li>
    
<li><a href="#InputFormats_and_MapReduce_Components">InputFormats and MapReduce Components</a></li>
  </ul></li>
  
<li><a href="#Appendix">Appendix</a>
  
<ul>
    
<li><a href="#Map_sizing">Map sizing</a></li>
    
<li><a href="#Copying_Between_Versions_of_HDFS">Copying Between Versions of HDFS</a></li>
    
<li><a href="#MapReduce_and_other_side-effects">MapReduce and other side-effects</a></li>
    
<li><a href="#SSL_Configurations_for_HSFTP_sources">SSL Configurations for HSFTP sources</a></li>
  </ul></li>
  
<li><a href="#Frequently_Asked_Questions">Frequently Asked Questions</a></li>
</ul>
<hr />
<div class="section">
<h2>Overview<a name="Overview"></a></h2>
<p>DistCp Version 2 (distributed copy) is a tool used for large  inter/intra-cluster copying. It uses MapReduce to effect its distribution,  error handling and recovery, and reporting. It expands a list of files and  directories into input to map tasks, each of which will copy a partition of  the files specified in the source list.</p>
<p><a class="externalLink" href="http://hadoop.apache.org/docs/r1.2.1/distcp.html">The erstwhile implementation of DistCp</a> has its share of quirks  and drawbacks, both in its usage, as well as its extensibility and  performance. The purpose of the DistCp refactor was to fix these  shortcomings, enabling it to be used and extended programmatically. New  paradigms have been introduced to improve runtime and setup performance,  while simultaneously retaining the legacy behaviour as default.</p>
<p>This document aims to describe the design of the new DistCp, its spanking new  features, their optimal use, and any deviance from the legacy implementation.</p></div>
<div class="section">
<h2>Usage<a name="Usage"></a></h2>
<div class="section">
<h3>Basic Usage<a name="Basic_Usage"></a></h3>
<p>The most common invocation of DistCp is an inter-cluster copy:</p>

<div class="source">
<pre>bash$ hadoop distcp hdfs://nn1:8020/foo/bar \
hdfs://nn2:8020/bar/foo
</pre></div>
<p>This will expand the namespace under <tt>/foo/bar</tt> on nn1 into a temporary file,  partition its contents among a set of map tasks, and start a copy on each  NodeManager from nn1 to nn2.</p>
<p>One can also specify multiple source directories on the command line:</p>

<div class="source">
<pre>bash$ hadoop distcp hdfs://nn1:8020/foo/a \
hdfs://nn1:8020/foo/b \
hdfs://nn2:8020/bar/foo
</pre></div>
<p>Or, equivalently, from a file using the -f option:</p>

<div class="source">
<pre>bash$ hadoop distcp -f hdfs://nn1:8020/srclist \
hdfs://nn2:8020/bar/foo
</pre></div>
<p>Where <tt>srclist</tt> contains</p>

<div class="source">
<pre>hdfs://nn1:8020/foo/a
hdfs://nn1:8020/foo/b
</pre></div>
<p>When copying from multiple sources, DistCp will abort the copy with an error  message if two sources collide, but collisions at the destination are  resolved per the <a href="#Command_Line_Options">options</a> specified. By default,  files already existing at the destination are skipped (i.e. not replaced by  the source file). A count of skipped files is reported at the end of each  job, but it may be inaccurate if a copier failed for some subset of its  files, but succeeded on a later attempt.</p>
<p>It is important that each NodeManager can reach and communicate with both the  source and destination file systems. For HDFS, both the source and  destination must be running the same version of the protocol or use a  backwards-compatible protocol; see <a href="#Copying_Between_Versions_of_HDFS">Copying Between Versions</a>.</p>
<p>After a copy, it is recommended that one generates and cross-checks a listing  of the source and destination to verify that the copy was truly successful.  Since DistCp employs both Map/Reduce and the FileSystem API, issues in or  between any of the three could adversely and silently affect the copy. Some  have had success running with <tt>-update</tt> enabled to perform a second pass, but  users should be acquainted with its semantics before attempting this.</p>
<p>It&#x2019;s also worth noting that if another client is still writing to a source  file, the copy will likely fail. Attempting to overwrite a file being written  at the destination should also fail on HDFS. If a source file is (re)moved  before it is copied, the copy will fail with a FileNotFoundException.</p>
<p>Please refer to the detailed Command Line Reference for information on all  the options available in DistCp.</p></div>
<div class="section">
<h3>Update and Overwrite<a name="Update_and_Overwrite"></a></h3>
<p><tt>-update</tt> is used to copy files from source that don&#x2019;t exist at the target  or differ from the target version. <tt>-overwrite</tt> overwrites target-files that  exist at the target.</p>
<p>The Update and Overwrite options warrant special attention since their  handling of source-paths varies from the defaults in a very subtle manner.  Consider a copy from <tt>/source/first/</tt> and <tt>/source/second/</tt> to <tt>/target/</tt>,  where the source paths have the following contents:</p>

<div class="source">
<pre>hdfs://nn1:8020/source/first/1
hdfs://nn1:8020/source/first/2
hdfs://nn1:8020/source/second/10
hdfs://nn1:8020/source/second/20
</pre></div>
<p>When DistCp is invoked without <tt>-update</tt> or <tt>-overwrite</tt>, the DistCp defaults  would create directories <tt>first/</tt> and <tt>second/</tt>, under <tt>/target</tt>. Thus:</p>

<div class="source">
<pre>distcp hdfs://nn1:8020/source/first hdfs://nn1:8020/source/second hdfs://nn2:8020/target
</pre></div>
<p>would yield the following contents in <tt>/target</tt>:</p>

<div class="source">
<pre>hdfs://nn2:8020/target/first/1
hdfs://nn2:8020/target/first/2
hdfs://nn2:8020/target/second/10
hdfs://nn2:8020/target/second/20
</pre></div>
<p>When either <tt>-update</tt> or <tt>-overwrite</tt> is specified, the <b>contents</b> of the  source-directories are copied to target, and not the source directories  themselves. Thus:</p>

<div class="source">
<pre>distcp -update hdfs://nn1:8020/source/first hdfs://nn1:8020/source/second hdfs://nn2:8020/target
</pre></div>
<p>would yield the following contents in <tt>/target</tt>:</p>

<div class="source">
<pre>hdfs://nn2:8020/target/1
hdfs://nn2:8020/target/2
hdfs://nn2:8020/target/10
hdfs://nn2:8020/target/20
</pre></div>
<p>By extension, if both source folders contained a file with the same name  (say, <tt>0</tt>), then both sources would map an entry to <tt>/target/0</tt> at the  destination. Rather than to permit this conflict, DistCp will abort.</p>
<p>Now, consider the following copy operation:</p>

<div class="source">
<pre>distcp hdfs://nn1:8020/source/first hdfs://nn1:8020/source/second hdfs://nn2:8020/target
</pre></div>
<p>With sources/sizes:</p>

<div class="source">
<pre>hdfs://nn1:8020/source/first/1 32
hdfs://nn1:8020/source/first/2 32
hdfs://nn1:8020/source/second/10 64
hdfs://nn1:8020/source/second/20 32
</pre></div>
<p>And destination/sizes:</p>

<div class="source">
<pre>hdfs://nn2:8020/target/1 32
hdfs://nn2:8020/target/10 32
hdfs://nn2:8020/target/20 64
</pre></div>
<p>Will effect:</p>

<div class="source">
<pre>hdfs://nn2:8020/target/1 32
hdfs://nn2:8020/target/2 32
hdfs://nn2:8020/target/10 64
hdfs://nn2:8020/target/20 32
</pre></div>
<p><tt>1</tt> is skipped because the file-length and contents match. <tt>2</tt> is copied  because it doesn&#x2019;t exist at the target. <tt>10</tt> and <tt>20</tt> are overwritten since  the contents don&#x2019;t match the source.</p>
<p>If <tt>-update</tt> is used, <tt>1</tt> is overwritten as well.</p></div>
<div class="section">
<h3>raw Namespace Extended Attribute Preservation<a name="raw_Namespace_Extended_Attribute_Preservation"></a></h3>
<p>This section only applies to HDFS.</p>
<p>If the target and all of the source pathnames are in the /.reserved/raw  hierarchy, then &#x2018;raw&#x2019; namespace extended attributes will be preserved.  &#x2018;raw&#x2019; xattrs are used by the system for internal functions such as encryption  meta data. They are only visible to users when accessed through the  /.reserved/raw hierarchy.</p>
<p>raw xattrs are preserved based solely on whether /.reserved/raw prefixes are  supplied. The -p (preserve, see below) flag does not impact preservation of  raw xattrs.</p>
<p>To prevent raw xattrs from being preserved, simply do not use the  /.reserved/raw prefix on any of the source and target paths.</p>
<p>If the /.reserved/raw prefix is specified on only a subset of the source and  target paths, an error will be displayed and a non-0 exit code returned.</p></div></div>
<div class="section">
<h2>Command Line Options<a name="Command_Line_Options"></a></h2>

<table border="0" class="bodyTable">
  <thead>
    
<tr class="a">
      
<th>Flag </th>
      
<th>Description </th>
      
<th>Notes</th>
    </tr>
  </thead>
  <tbody>
    
<tr class="b">
      
<td><tt>-p[rbugpcax]</tt> </td>
      
<td>Preserve r: replication number b: block size u: user g: group p: permission c: checksum-type a: ACL x: XAttr </td>
      
<td>Modification times are not preserved. Also, when <tt>-update</tt> is specified, status updates will <b>not</b> be synchronized unless the file sizes also differ (i.e. unless the file is re-created). If -pa is specified, DistCp preserves the permissions also because ACLs are a super-set of permissions.</td>
    </tr>
    
<tr class="a">
      
<td><tt>-i</tt> </td>
      
<td>Ignore failures </td>
      
<td>As explained in the Appendix, this option will keep more accurate statistics about the copy than the default case. It also preserves logs from failed copies, which can be valuable for debugging. Finally, a failing map will not cause the job to fail before all splits are attempted.</td>
    </tr>
    
<tr class="b">
      
<td><tt>-log &lt;logdir&gt;</tt> </td>
      
<td>Write logs to &lt;logdir&gt; </td>
      
<td>DistCp keeps logs of each file it attempts to copy as map output. If a map fails, the log output will not be retained if it is re-executed.</td>
    </tr>
    
<tr class="a">
      
<td><tt>-m &lt;num_maps&gt;</tt> </td>
      
<td>Maximum number of simultaneous copies </td>
      
<td>Specify the number of maps to copy data. Note that more maps may not necessarily improve throughput.</td>
    </tr>
    
<tr class="b">
      
<td><tt>-overwrite</tt> </td>
      
<td>Overwrite destination </td>
      
<td>If a map fails and <tt>-i</tt> is not specified, all the files in the split, not only those that failed, will be recopied. As discussed in the Usage documentation, it also changes the semantics for generating destination paths, so users should use this carefully.</td>
    </tr>
    
<tr class="a">
      
<td><tt>-update</tt> </td>
      
<td>Overwrite if source and destination differ in size, blocksize, or checksum </td>
      
<td>As noted in the preceding, this is not a &#x201c;sync&#x201d; operation. The criteria examined are the source and destination file sizes, blocksizes, and checksums; if they differ, the source file replaces the destination file. As discussed in the Usage documentation, it also changes the semantics for generating destination paths, so users should use this carefully.</td>
    </tr>
    
<tr class="b">
      
<td><tt>-f &lt;urilist_uri&gt;</tt> </td>
      
<td>Use list at &lt;urilist_uri&gt; as src list </td>
      
<td>This is equivalent to listing each source on the command line. The <tt>urilist_uri</tt> list should be a fully qualified URI.</td>
    </tr>
    
<tr class="a">
      
<td><tt>-filelimit &lt;n&gt;</tt> </td>
      
<td>Limit the total number of files to be &lt;= n </td>
      
<td><b>Deprecated!</b> Ignored in the new DistCp.</td>
    </tr>
    
<tr class="b">
      
<td><tt>-sizelimit &lt;n&gt;</tt> </td>
      
<td>Limit the total size to be &lt;= n bytes </td>
      
<td><b>Deprecated!</b> Ignored in the new DistCp.</td>
    </tr>
    
<tr class="a">
      
<td><tt>-delete</tt> </td>
      
<td>Delete the files existing in the dst but not in src </td>
      
<td>The deletion is done by FS Shell. So the trash will be used, if it is enable.</td>
    </tr>
    
<tr class="b">
      
<td><tt>-strategy {dynamic|uniformsize}</tt> </td>
      
<td>Choose the copy-strategy to be used in DistCp. </td>
      
<td>By default, uniformsize is used. (i.e. Maps are balanced on the total size of files copied by each map. Similar to legacy.) If &#x201c;dynamic&#x201d; is specified, <tt>DynamicInputFormat</tt> is used instead. (This is described in the Architecture section, under InputFormats.)</td>
    </tr>
    
<tr class="a">
      
<td><tt>-bandwidth</tt> </td>
      
<td>Specify bandwidth per map, in MB/second. </td>
      
<td>Each map will be restricted to consume only the specified bandwidth. This is not always exact. The map throttles back its bandwidth consumption during a copy, such that the <b>net</b> bandwidth used tends towards the specified value.</td>
    </tr>
    
<tr class="b">
      
<td><tt>-atomic {-tmp &lt;tmp_dir&gt;}</tt> </td>
      
<td>Specify atomic commit, with optional tmp directory. </td>
      
<td><tt>-atomic</tt> instructs DistCp to copy the source data to a temporary target location, and then move the temporary target to the final-location atomically. Data will either be available at final target in a complete and consistent form, or not at all. Optionally, <tt>-tmp</tt> may be used to specify the location of the tmp-target. If not specified, a default is chosen. <b>Note:</b> tmp_dir must be on the final target cluster.</td>
    </tr>
    
<tr class="a">
      
<td><tt>-mapredSslConf &lt;ssl_conf_file&gt;</tt> </td>
      
<td>Specify SSL Config file, to be used with HSFTP source </td>
      
<td>When using the hsftp protocol with a source, the security- related properties may be specified in a config-file and passed to DistCp. &lt;ssl_conf_file&gt; needs to be in the classpath.</td>
    </tr>
    
<tr class="b">
      
<td><tt>-async</tt> </td>
      
<td>Run DistCp asynchronously. Quits as soon as the Hadoop Job is launched. </td>
      
<td>The Hadoop Job-id is logged, for tracking.</td>
    </tr>
  </tbody>
</table></div>
<div class="section">
<h2>Architecture of DistCp<a name="Architecture_of_DistCp"></a></h2>
<p>The components of the new DistCp may be classified into the following  categories:</p>

<ul>
  
<li>DistCp Driver</li>
  
<li>Copy-listing generator</li>
  
<li>Input-formats and Map-Reduce components</li>
</ul>
<div class="section">
<h3>DistCp Driver<a name="DistCp_Driver"></a></h3>
<p>The DistCp Driver components are responsible for:</p>

<ul>
  
<li>
<p>Parsing the arguments passed to the DistCp command on the command-line, via:</p>
  
<ul>
    
<li>OptionsParser, and</li>
    
<li>DistCpOptionsSwitch</li>
  </ul></li>
  
<li>
<p>Assembling the command arguments into an appropriate DistCpOptions object, and initializing DistCp. These arguments include:</p>
  
<ul>
    
<li>Source-paths</li>
    
<li>Target location</li>
    
<li>Copy options (e.g. whether to update-copy, overwrite, which  file-attributes to preserve, etc.)</li>
  </ul></li>
  
<li>
<p>Orchestrating the copy operation by:</p>
  
<ul>
    
<li>Invoking the copy-listing-generator to create the list of files to be  copied.</li>
    
<li>Setting up and launching the Hadoop Map-Reduce Job to carry out the  copy.</li>
    
<li>Based on the options, either returning a handle to the Hadoop MR Job  immediately, or waiting till completion.</li>
  </ul></li>
</ul>
<p>The parser-elements are exercised only from the command-line (or if  DistCp::run() is invoked). The DistCp class may also be used  programmatically, by constructing the DistCpOptions object, and initializing  a DistCp object appropriately.</p></div>
<div class="section">
<h3>Copy-listing Generator<a name="Copy-listing_Generator"></a></h3>
<p>The copy-listing-generator classes are responsible for creating the list of  files/directories to be copied from source. They examine the contents of the  source-paths (files/directories, including wild-cards), and record all paths  that need copy into a SequenceFile, for consumption by the DistCp Hadoop  Job. The main classes in this module include:</p>

<ol style="list-style-type: decimal">
  
<li>CopyListing: The interface that should be implemented by any  copy-listing-generator implementation. Also provides the factory method by  which the concrete CopyListing implementation is chosen.</li>
  
<li>SimpleCopyListing: An implementation of CopyListing that accepts multiple  source paths (files/directories), and recursively lists all the individual  files and directories under each, for copy.</li>
  
<li>GlobbedCopyListing: Another implementation of CopyListing that expands  wild-cards in the source paths.</li>
  
<li>FileBasedCopyListing: An implementation of CopyListing that reads the  source-path list from a specified file.</li>
</ol>
<p>Based on whether a source-file-list is specified in the DistCpOptions, the  source-listing is generated in one of the following ways:</p>

<ol style="list-style-type: decimal">
  
<li>If there&#x2019;s no source-file-list, the GlobbedCopyListing is used. All  wild-cards are expanded, and all the expansions are forwarded to the  SimpleCopyListing, which in turn constructs the listing (via recursive  descent of each path).</li>
  
<li>If a source-file-list is specified, the FileBasedCopyListing is used.  Source-paths are read from the specified file, and then forwarded to the  GlobbedCopyListing. The listing is then constructed as described above.</li>
</ol>
<p>One may customize the method by which the copy-listing is constructed by  providing a custom implementation of the CopyListing interface. The behaviour  of DistCp differs here from the legacy DistCp, in how paths are considered  for copy.</p>
<p>The legacy implementation only lists those paths that must definitely be  copied on to target. E.g. if a file already exists at the target (and  <tt>-overwrite</tt> isn&#x2019;t specified), the file isn&#x2019;t even considered in the  MapReduce Copy Job. Determining this during setup (i.e. before the MapReduce  Job) involves file-size and checksum-comparisons that are potentially  time-consuming.</p>
<p>The new DistCp postpones such checks until the MapReduce Job, thus reducing  setup time. Performance is enhanced further since these checks are  parallelized across multiple maps.</p></div>
<div class="section">
<h3>InputFormats and MapReduce Components<a name="InputFormats_and_MapReduce_Components"></a></h3>
<p>The InputFormats and MapReduce components are responsible for the actual copy  of files and directories from the source to the destination path. The  listing-file created during copy-listing generation is consumed at this  point, when the copy is carried out. The classes of interest here include:</p>

<ul>
  
<li>
<p><b>UniformSizeInputFormat:</b> This implementation of org.apache.hadoop.mapreduce.InputFormat provides equivalence with Legacy DistCp in balancing load across maps. The aim of the UniformSizeInputFormat is to make each map copy roughly the same number of bytes. Apropos, the listing file is split into groups of paths, such that the sum of file-sizes in each InputSplit is nearly equal to every other map. The splitting isn&#x2019;t always perfect, but its trivial implementation keeps the setup-time low.</p></li>
  
<li>
<p><b>DynamicInputFormat and DynamicRecordReader:</b> The DynamicInputFormat implements org.apache.hadoop.mapreduce.InputFormat, and is new to DistCp. The listing-file is split into several &#x201c;chunk-files&#x201d;, the exact number of chunk-files being a multiple of the number of maps requested for in the Hadoop Job. Each map task is &#x201c;assigned&#x201d; one of the chunk-files (by renaming the chunk to the task&#x2019;s id), before the Job is launched. Paths are read from each chunk using the DynamicRecordReader, and processed in the CopyMapper. After all the paths in a chunk are processed, the current chunk is deleted and a new chunk is acquired. The process continues until no more chunks are available. This &#x201c;dynamic&#x201d; approach allows faster map-tasks to consume more paths than slower ones, thus speeding up the DistCp job overall.</p></li>
  
<li>
<p><b>CopyMapper:</b> This class implements the physical file-copy. The input-paths are checked against the input-options (specified in the Job&#x2019;s Configuration), to determine whether a file needs copy. A file will be copied only if at least one of the following is true:</p>
  
<ul>
    
<li>A file with the same name doesn&#x2019;t exist at target.</li>
    
<li>A file with the same name exists at target, but has a different file  size.</li>
    
<li>A file with the same name exists at target, but has a different  checksum, and <tt>-skipcrccheck</tt> isn&#x2019;t mentioned.</li>
    
<li>A file with the same name exists at target, but <tt>-overwrite</tt> is  specified.</li>
    
<li>A file with the same name exists at target, but differs in block-size  (and block-size needs to be preserved.</li>
  </ul></li>
  
<li>
<p><b>CopyCommitter:</b> This class is responsible for the commit-phase of the DistCp job, including:</p>
  
<ul>
    
<li>Preservation of directory-permissions (if specified in the options)</li>
    
<li>Clean-up of temporary-files, work-directories, etc.</li>
  </ul></li>
</ul></div></div>
<div class="section">
<h2>Appendix<a name="Appendix"></a></h2>
<div class="section">
<h3>Map sizing<a name="Map_sizing"></a></h3>
<p>By default, DistCp makes an attempt to size each map comparably so that each  copies roughly the same number of bytes. Note that files are the finest level  of granularity, so increasing the number of simultaneous copiers (i.e. maps)  may not always increase the number of simultaneous copies nor the overall  throughput.</p>
<p>The new DistCp also provides a strategy to &#x201c;dynamically&#x201d; size maps, allowing  faster data-nodes to copy more bytes than slower nodes. Using <tt>-strategy
  dynamic</tt> (explained in the Architecture), rather than to assign a fixed set  of source-files to each map-task, files are instead split into several sets.  The number of sets exceeds the number of maps, usually by a factor of 2-3.  Each map picks up and copies all files listed in a chunk. When a chunk is  exhausted, a new chunk is acquired and processed, until no more chunks  remain.</p>
<p>By not assigning a source-path to a fixed map, faster map-tasks (i.e.  data-nodes) are able to consume more chunks, and thus copy more data, than  slower nodes. While this distribution isn&#x2019;t uniform, it is fair with regard  to each mapper&#x2019;s capacity.</p>
<p>The dynamic-strategy is implemented by the DynamicInputFormat. It provides  superior performance under most conditions.</p>
<p>Tuning the number of maps to the size of the source and destination clusters,  the size of the copy, and the available bandwidth is recommended for  long-running and regularly run jobs.</p></div>
<div class="section">
<h3>Copying Between Versions of HDFS<a name="Copying_Between_Versions_of_HDFS"></a></h3>
<p>For copying between two different versions of Hadoop, one will usually use  HftpFileSystem. This is a read-only FileSystem, so DistCp must be run on the  destination cluster (more specifically, on NodeManagers that can write to the  destination cluster). Each source is specified as  <tt>hftp://&lt;dfs.http.address&gt;/&lt;path&gt;</tt> (the default <tt>dfs.http.address</tt> is  <tt>&lt;namenode&gt;:50070</tt>).</p></div>
<div class="section">
<h3>MapReduce and other side-effects<a name="MapReduce_and_other_side-effects"></a></h3>
<p>As has been mentioned in the preceding, should a map fail to copy one of its  inputs, there will be several side-effects.</p>

<ul>
  
<li>Unless <tt>-overwrite</tt> is specified, files successfully copied by a previous map on a re-execution will be marked as &#x201c;skipped&#x201d;.</li>
  
<li>If a map fails <tt>mapreduce.map.maxattempts</tt> times, the remaining map tasks will be killed (unless <tt>-i</tt> is set).</li>
  
<li>If <tt>mapreduce.map.speculative</tt> is set set final and true, the result of the copy is undefined.</li>
</ul></div>
<div class="section">
<h3>SSL Configurations for HSFTP sources<a name="SSL_Configurations_for_HSFTP_sources"></a></h3>
<p>To use an HSFTP source (i.e. using the hsftp protocol), a SSL configuration  file needs to be specified (via the <tt>-mapredSslConf</tt> option). This must  specify 3 parameters:</p>

<ul>
  
<li><tt>ssl.client.truststore.location</tt>: The local-filesystem location of the trust-store file, containing the certificate for the NameNode.</li>
  
<li><tt>ssl.client.truststore.type</tt>: (Optional) The format of the trust-store file.</li>
  
<li><tt>ssl.client.truststore.password</tt>: (Optional) Password for the trust-store file.</li>
</ul>
<p>The following is an example of the contents of the contents of a SSL  Configuration file:</p>

<div class="source">
<pre>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;ssl.client.truststore.location&lt;/name&gt;
    &lt;value&gt;/work/keystore.jks&lt;/value&gt;
    &lt;description&gt;Truststore to be used by clients like distcp. Must be specified.&lt;/description&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;ssl.client.truststore.password&lt;/name&gt;
    &lt;value&gt;changeme&lt;/value&gt;
    &lt;description&gt;Optional. Default value is &quot;&quot;.&lt;/description&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;ssl.client.truststore.type&lt;/name&gt;
    &lt;value&gt;jks&lt;/value&gt;
    &lt;description&gt;Optional. Default value is &quot;jks&quot;.&lt;/description&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</pre></div>
<p>The SSL configuration file must be in the class-path of the DistCp program.</p></div></div>
<div class="section">
<h2>Frequently Asked Questions<a name="Frequently_Asked_Questions"></a></h2>

<ol style="list-style-type: decimal">
  
<li>
<p><b>Why does -update not create the parent source-directory under a pre-existing target directory?</b>  The behaviour of <tt>-update</tt> and <tt>-overwrite</tt> is described in detail in the  Usage section of this document. In short, if either option is used with a  pre-existing destination directory, the <b>contents</b> of each source  directory is copied over, rather than the source-directory itself. This  behaviour is consistent with the legacy DistCp implementation as well.</p></li>
  
<li>
<p><b>How does the new DistCp differ in semantics from the Legacy DistCp?</b></p>
  
<ul>
    
<li>Files that are skipped during copy used to also have their  file-attributes (permissions, owner/group info, etc.) unchanged, when  copied with Legacy DistCp. These are now updated, even if the file-copy  is skipped.</li>
    
<li>Empty root directories among the source-path inputs were not created at  the target, in Legacy DistCp. These are now created.</li>
  </ul></li>
  
<li>
<p><b>Why does the new DistCp use more maps than legacy DistCp?</b>  Legacy DistCp works by figuring out what files need to be actually copied  to target before the copy-job is launched, and then launching as many maps  as required for copy. So if a majority of the files need to be skipped  (because they already exist, for example), fewer maps will be needed. As a  consequence, the time spent in setup (i.e. before the M/R job) is higher.  The new DistCp calculates only the contents of the source-paths. It  doesn&#x2019;t try to filter out what files can be skipped. That decision is put  off till the M/R job runs. This is much faster (vis-a-vis execution-time),  but the number of maps launched will be as specified in the <tt>-m</tt> option,  or 20 (default) if unspecified.</p></li>
  
<li>
<p><b>Why does DistCp not run faster when more maps are specified?</b>  At present, the smallest unit of work for DistCp is a file. i.e., a file  is processed by only one map. Increasing the number of maps to a value  exceeding the number of files would yield no performance benefit. The  number of maps launched would equal the number of files.</p></li>
  
<li>
<p><b>Why does DistCp run out of memory?</b>  If the number of individual files/directories being copied from the source  path(s) is extremely large (e.g. 1,000,000 paths), DistCp might run out of  memory while determining the list of paths for copy. This is not unique to  the new DistCp implementation.  To get around this, consider changing the <tt>-Xmx</tt> JVM heap-size parameters,  as follows:</p>
  
<div class="source">
<pre> bash$ export HADOOP_CLIENT_OPTS=&quot;-Xms64m -Xmx1024m&quot;
 bash$ hadoop distcp /source /target
</pre></div></li>
</ol></div>
      </div>
    </div>
    <div class="clear">
      <hr/>
    </div>
    <div id="footer">
      <div class="xright">&#169;            2015
              Apache Software Foundation
            
                       - <a href="http://maven.apache.org/privacy-policy.html">Privacy Policy</a></div>
      <div class="clear">
        <hr/>
      </div>
    </div>
  </body>
</html>
