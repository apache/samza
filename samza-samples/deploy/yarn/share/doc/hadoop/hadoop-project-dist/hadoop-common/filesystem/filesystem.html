<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- Generated by Apache Maven Doxia at 2015-09-16 -->
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Apache Hadoop 2.6.1 - </title>
    <style type="text/css" media="all">
      @import url("../css/maven-base.css");
      @import url("../css/maven-theme.css");
      @import url("../css/site.css");
    </style>
    <link rel="stylesheet" href="../css/print.css" type="text/css" media="print" />
        <meta name="Date-Revision-yyyymmdd" content="20150916" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      </head>
  <body class="composite">
    <div id="banner">
                  <a href="http://hadoop.apache.org/" id="bannerLeft">
                                        <img src="http://hadoop.apache.org/images/hadoop-logo.jpg" alt="" />
                </a>
                        <a href="http://www.apache.org/" id="bannerRight">
                                        <img src="http://www.apache.org/images/asf_logo_wide.png" alt="" />
                </a>
            <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="breadcrumbs">
            
                                <div class="xleft">
                          <a href="http://www.apache.org/" class="externalLink">Apache</a>
                &gt;
                      <a href="http://hadoop.apache.org/" class="externalLink">Hadoop</a>
                &gt;
                      <a href="../../">Apache Hadoop Project Dist POM</a>
                &gt;
                      <a href="../">Apache Hadoop 2.6.1</a>
                  </div>
            <div class="xright">            <a href="http://wiki.apache.org/hadoop" class="externalLink">Wiki</a>
            |
                <a href="https://svn.apache.org/repos/asf/hadoop/" class="externalLink">SVN</a>
            |
                <a href="http://hadoop.apache.org/" class="externalLink">Apache Hadoop</a>
              
                                &nbsp;| Last Published: 2015-09-16
              &nbsp;| Version: 2.6.1
            </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="leftColumn">
      <div id="navcolumn">
             
                                                <h5>General</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/SingleCluster.html">Single Node Setup</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/ClusterSetup.html">Cluster Setup</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CommandsManual.html">Hadoop Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/FileSystemShell.html">FileSystem Shell</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Compatibility.html">Hadoop Compatibility</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/filesystem/index.html">FileSystem Specification</a>
            </li>
          </ul>
                       <h5>Common</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CLIMiniCluster.html">CLI Mini Cluster</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/NativeLibraries.html">Native Libraries</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Superusers.html">Superusers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/SecureMode.html">Secure Mode</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/ServiceLevelAuth.html">Service Level Authorization</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/HttpAuthentication.html">HTTP Authentication</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-kms/index.html">Hadoop KMS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Tracing.html">Tracing</a>
            </li>
          </ul>
                       <h5>HDFS</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html">HDFS User Guide</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSCommands.html">HDFS Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">High Availability With QJM</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">High Availability With NFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/Federation.html">Federation</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ViewFs.html">ViewFs Guide</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">HDFS Snapshots</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsDesign.html">HDFS Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsEditsViewer.html">Edits Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsImageViewer.html">Image Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">Permissions and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html">Quotas and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/Hftp.html">HFTP</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/LibHdfs.html">C API libhdfs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/WebHDFS.html">WebHDFS REST API</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-hdfs-httpfs/index.html">HttpFS Gateway</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html">Short Circuit Local Reads</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html">Centralized Cache Management</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html">HDFS NFS Gateway</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html">HDFS Rolling Upgrade</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ExtendedAttributes.html">Extended Attributes</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html">Transparent Encryption</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsMultihoming.html">HDFS Support for Multihoming</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html">Archival Storage, SSD & Memory</a>
            </li>
          </ul>
                       <h5>MapReduce</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">MapReduce Tutorial</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredCommands.html">MapReduce Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html">Compatibilty between Hadoop 1.x and Hadoop 2.x</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/EncryptedShuffle.html">Encrypted Shuffle</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/PluggableShuffleAndPluggableSort.html">Pluggable Shuffle/Sort</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistributedCacheDeploy.html">Distributed Cache Deploy</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/HadoopStreaming.html">Hadoop Streaming</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/HadoopArchives.html">Hadoop Archives</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistCp.html">DistCp</a>
            </li>
          </ul>
                       <h5>MapReduce REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredAppMasterRest.html">MR Application Master</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html">MR History Server</a>
            </li>
          </ul>
                       <h5>YARN</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YARN.html">YARN Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">Capacity Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/FairScheduler.html">Fair Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html">ResourceManager Restart</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">ResourceManager HA</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WebApplicationProxy.html">Web Application Proxy</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html">YARN Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html">Writing YARN Applications</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YarnCommands.html">YARN Commands</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-sls/SchedulerLoadSimulator.html">Scheduler Load Simulator</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManagerRestart.html">NodeManager Restart</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/DockerContainerExecutor.html">DockerContainerExecutor</a>
            </li>
          </ul>
                       <h5>YARN REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html">Introduction</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">Resource Manager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManagerRest.html">Node Manager</a>
            </li>
          </ul>
                       <h5>Auth</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-auth/index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/Examples.html">Examples</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/Configuration.html">Configuration</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/BuildingIt.html">Building</a>
            </li>
          </ul>
                       <h5>Reference</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/releasenotes.html">Release Notes</a>
            </li>
                  <li class="none">
                  <a href="../../../api/index.html">API docs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CHANGES.txt">Common CHANGES.txt</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/CHANGES.txt">HDFS CHANGES.txt</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-mapreduce/CHANGES.txt">MapReduce CHANGES.txt</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-yarn/CHANGES.txt">YARN CHANGES.txt</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Metrics.html">Metrics</a>
            </li>
          </ul>
                       <h5>Configuration</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/core-default.xml">core-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">hdfs-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">mapred-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/DeprecatedProperties.html">Deprecated Properties</a>
            </li>
          </ul>
                                 <a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy">
          <img alt="Built by Maven" src="../images/logos/maven-feather.png"/>
        </a>
                       
                            </div>
    </div>
    <div id="bodyColumn">
      <div id="contentBox">
        <!-- -
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file. -->
<!-- ============================================================= -->
<!-- CLASS: FileSystem -->
<!-- ============================================================= --><h1>class <tt>org.apache.hadoop.fs.FileSystem</tt></h1>
<p>The abstract <tt>FileSystem</tt> class is the original class to access Hadoop filesystems; non-abstract subclasses exist for all Hadoop-supported filesystems.</p>
<p>All operations that take a Path to this interface MUST support relative paths. In such a case, they must be resolved relative to the working directory defined by <tt>setWorkingDirectory()</tt>.</p>
<p>For all clients, therefore, we also add the notion of a state component PWD: this represents the present working directory of the client. Changes to this state are not reflected in the filesystem itself: they are unique to the instance of the client.</p>
<p><b>Implementation Note</b>: the static <tt>FileSystem get(URI uri, Configuration conf)</tt> method MAY return a pre-existing instance of a filesystem client class&#x2014;a class that may also be in use in other threads. The implementations of <tt>FileSystem</tt> which ship with Apache Hadoop <i>do not make any attempt to synchronize access to the working directory field</i>.</p>
<div class="section">
<h2>Invariants<a name="Invariants"></a></h2>
<p>All the requirements of a valid FileSystem are considered implicit preconditions and postconditions: all operations on a valid FileSystem MUST result in a new FileSystem that is also valid.</p></div>
<div class="section">
<h2>Predicates and other state access operations<a name="Predicates_and_other_state_access_operations"></a></h2>
<div class="section">
<h3><tt>boolean exists(Path p)</tt><a name="boolean_existsPath_p"></a></h3>

<div class="source">
<pre>def exists(FS, p) = p in paths(FS)
</pre></div></div>
<div class="section">
<h3><tt>boolean isDirectory(Path p)</tt><a name="boolean_isDirectoryPath_p"></a></h3>

<div class="source">
<pre>def isDirectory(FS, p)= p in directories(FS)
</pre></div></div>
<div class="section">
<h3><tt>boolean isFile(Path p)</tt><a name="boolean_isFilePath_p"></a></h3>

<div class="source">
<pre>def isFile(FS, p) = p in files(FS)
</pre></div></div>
<div class="section">
<h3><tt>boolean isSymlink(Path p)</tt><a name="boolean_isSymlinkPath_p"></a></h3>

<div class="source">
<pre>def isSymlink(FS, p) = p in symlinks(FS)
</pre></div></div>
<div class="section">
<h3>&#x2018;boolean inEncryptionZone(Path p)&#x2019;<a name="aboolean_inEncryptionZonePath_p"></a></h3>
<p>Return True if the data for p is encrypted. The nature of the encryption and the mechanism for creating an encryption zone are implementation details not covered in this specification. No guarantees are made about the quality of the encryption. The metadata is not encrypted.</p>
<div class="section">
<h4>Preconditions<a name="Preconditions"></a></h4>

<div class="source">
<pre>if not exists(FS, p) : raise FileNotFoundException
</pre></div></div>
<div class="section">
<h4>Postconditions<a name="Postconditions"></a></h4></div>
<div class="section">
<h4>Invariants<a name="Invariants"></a></h4>
<p>All files and directories under a directory in an encryption zone are also in an encryption zone</p>

<div class="source">
<pre>forall d in directories(FS): inEncyptionZone(FS, d) implies
  forall c in children(FS, d) where (isFile(FS, c) or isDir(FS, c)) :
    inEncyptionZone(FS, c)
</pre></div>
<p>For all files in an encrypted zone, the data is encrypted, but the encryption type and specification are not defined.</p>

<div class="source">
<pre>  forall f in files(FS) where  inEncyptionZone(FS, c):
    isEncrypted(data(f))
</pre></div></div></div>
<div class="section">
<h3><tt>FileStatus getFileStatus(Path p)</tt><a name="FileStatus_getFileStatusPath_p"></a></h3>
<p>Get the status of a path</p>
<div class="section">
<h4>Preconditions<a name="Preconditions"></a></h4>

<div class="source">
<pre>if not exists(FS, p) : raise FileNotFoundException
</pre></div></div>
<div class="section">
<h4>Postconditions<a name="Postconditions"></a></h4>

<div class="source">
<pre>result = stat: FileStatus where:
    if isFile(FS, p) :
        stat.length = len(FS.Files[p])
        stat.isdir = False
    elif isDir(FS, p) :
        stat.length = 0
        stat.isdir = True
    elif isSymlink(FS, p) :
        stat.length = 0
        stat.isdir = False
        stat.symlink = FS.Symlinks[p]
    if inEncryptionZone(FS, p) :
        stat.isEncrypted = True
    else
        stat.isEncrypted = False
</pre></div></div></div>
<div class="section">
<h3><tt>Path getHomeDirectory()</tt><a name="Path_getHomeDirectory"></a></h3>
<p>The function <tt>getHomeDirectory</tt> returns the home directory for the FileSystem and the current user account.</p>
<p>For some FileSystems, the path is <tt>[&quot;/&quot;, &quot;users&quot;, System.getProperty(&quot;user-name&quot;)]</tt>.</p>
<p>However, for HDFS, the username is derived from the credentials used to authenticate the client with HDFS. This may differ from the local user account name.</p>
<p>**It is the responsibility of the FileSystem to determine the actual home directory of the caller.**</p>
<div class="section">
<h4>Preconditions<a name="Preconditions"></a></h4></div>
<div class="section">
<h4>Postconditions<a name="Postconditions"></a></h4>

<div class="source">
<pre>result = p where valid-path(FS, p)
</pre></div>
<p>There is no requirement that the path exists at the time the method was called, or, if it exists, that it points to a directory. However, code tends to assume that <tt>not isFile(FS, getHomeDirectory())</tt> holds to the extent that follow-on code may fail.</p></div>
<div class="section">
<h4>Implementation Notes<a name="Implementation_Notes"></a></h4>

<ul>
  
<li>The FTPFileSystem queries this value from the remote filesystem and may fail with a RuntimeException or subclass thereof if there is a connectivity problem. The time to execute the operation is not bounded.</li>
</ul></div></div>
<div class="section">
<h3><tt>FileSystem.listStatus(Path, PathFilter )</tt><a name="FileSystem.listStatusPath_PathFilter_"></a></h3>
<p>A <tt>PathFilter</tt> <tt>f</tt> is a predicate function that returns true iff the path <tt>p</tt> meets the filter&#x2019;s conditions.</p>
<div class="section">
<h4>Preconditions<a name="Preconditions"></a></h4>
<p>Path must exist:</p>

<div class="source">
<pre>if not exists(FS, p) : raise FileNotFoundException
</pre></div></div>
<div class="section">
<h4>Postconditions<a name="Postconditions"></a></h4>

<div class="source">
<pre>if isFile(FS, p) and f(p) :
    result = [getFileStatus(p)]

elif isFile(FS, p) and not f(P) :
    result = []

elif isDir(FS, p):
   result [getFileStatus(c) for c in children(FS, p) where f(c) == True]
</pre></div>
<p><b>Implicit invariant</b>: the contents of a <tt>FileStatus</tt> of a child retrieved via <tt>listStatus()</tt> are equal to those from a call of <tt>getFileStatus()</tt> to the same path:</p>

<div class="source">
<pre>forall fs in listStatus(Path) :
  fs == getFileStatus(fs.path)
</pre></div></div></div>
<div class="section">
<h3>Atomicity and Consistency<a name="Atomicity_and_Consistency"></a></h3>
<p>By the time the <tt>listStatus()</tt> operation returns to the caller, there is no guarantee that the information contained in the response is current. The details MAY be out of date, including the contents of any directory, the attributes of any files, and the existence of the path supplied.</p>
<p>The state of a directory MAY change during the evaluation process. This may be reflected in a listing that is split between the pre- and post-update FileSystem states.</p>

<ul>
  
<li>
<p>After an entry at path <tt>P</tt> is created, and before any other  changes are made to the FileSystem, <tt>listStatus(P)</tt> MUST  find the file and return its status.</p></li>
  
<li>
<p>After an entry at path <tt>P</tt> is deleted, <tt>listStatus(P)</tt> MUST  raise a <tt>FileNotFoundException</tt>.</p></li>
  
<li>
<p>After an entry at path <tt>P</tt> is created, and before any other  changes are made to the FileSystem, the result of <tt>listStatus(parent(P))</tt> SHOULD  include the value of <tt>getFileStatus(P)</tt>.</p></li>
  
<li>
<p>After an entry at path <tt>P</tt> is created, and before any other  changes are made to the FileSystem, the result of <tt>listStatus(parent(P))</tt> SHOULD  NOT include the value of <tt>getFileStatus(P)</tt>.</p></li>
</ul>
<p>This is not a theoretical possibility, it is observable in HDFS when a directory contains many thousands of files.</p>
<p>Consider a directory &#x201c;d&#x201d; with the contents:</p>

<div class="source">
<pre>a
part-0000001
part-0000002
...
part-9999999
</pre></div>
<p>If the number of files is such that HDFS returns a partial listing in each response, then, if a listing <tt>listStatus(&quot;d&quot;)</tt> takes place concurrently with the operation <tt>rename(&quot;d/a&quot;,&quot;d/z&quot;))</tt>, the result may be one of:</p>

<div class="source">
<pre>[a, part-0000001, ... , part-9999999]
[part-0000001, ... , part-9999999, z]

[a, part-0000001, ... , part-9999999, z]
[part-0000001, ... , part-9999999]
</pre></div>
<p>While this situation is likely to be a rare occurrence, it MAY happen. In HDFS these inconsistent views are only likely when listing a directory with many children.</p>
<p>Other filesystems may have stronger consistency guarantees, or return inconsistent data more readily.</p></div>
<div class="section">
<h3><tt>List[BlockLocation] getFileBlockLocations(FileStatus f, int s, int l)</tt><a name="ListBlockLocation_getFileBlockLocationsFileStatus_f_int_s_int_l"></a></h3>
<div class="section">
<h4>Preconditions<a name="Preconditions"></a></h4>

<div class="source">
<pre>if s &lt; 0 or l &lt; 0 : raise {HadoopIllegalArgumentException, InvalidArgumentException}
</pre></div>

<ul>
  
<li>HDFS throws <tt>HadoopIllegalArgumentException</tt> for an invalid offset or length; this extends <tt>IllegalArgumentException</tt>.</li>
</ul></div>
<div class="section">
<h4>Postconditions<a name="Postconditions"></a></h4>
<p>If the filesystem is location aware, it must return the list of block locations where the data in the range <tt>[s:s+l]</tt> can be found.</p>

<div class="source">
<pre>if f == null :
    result = null
elif f.getLen()) &lt;= s
    result = []
else result = [ locations(FS, b) for all b in blocks(FS, p, s, s+l)]
</pre></div>
<p>where</p>

<div class="source">
<pre>  def locations(FS, b) = a list of all locations of a block in the filesystem

  def blocks(FS, p, s, s +  l)  = a list of the blocks containing  data(FS, path)[s:s+l]
</pre></div>
<p>Note that that as <tt>length(FS, f)</tt> is defined as 0 if <tt>isDir(FS, f)</tt>, the result of <tt>getFileBlockLocations()</tt> on a directory is []</p>
<p>If the filesystem is not location aware, it SHOULD return</p>

<div class="source">
<pre>  [
    BlockLocation([&quot;localhost:50010&quot;] ,
              [&quot;localhost&quot;],
              [&quot;/default/localhost&quot;]
               0, F.getLen())
   ] ;
</pre></div>
<p>*A bug in Hadoop 1.0.3 means that a topology path of the same number of elements as the cluster topology MUST be provided, hence Filesystems SHOULD return that <tt>&quot;/default/localhost&quot;</tt> path</p></div></div>
<div class="section">
<h3><tt>getFileBlockLocations(Path P, int S, int L)</tt><a name="getFileBlockLocationsPath_P_int_S_int_L"></a></h3>
<div class="section">
<h4>Preconditions<a name="Preconditions"></a></h4>

<div class="source">
<pre>if p == null : raise NullPointerException
if not exists(FS, p) : raise FileNotFoundException
</pre></div></div>
<div class="section">
<h4>Postconditions<a name="Postconditions"></a></h4>

<div class="source">
<pre>result = getFileBlockLocations(getStatus(P), S, L)
</pre></div></div></div>
<div class="section">
<h3><tt>getDefaultBlockSize()</tt><a name="getDefaultBlockSize"></a></h3>
<div class="section">
<h4>Preconditions<a name="Preconditions"></a></h4></div>
<div class="section">
<h4>Postconditions<a name="Postconditions"></a></h4>

<div class="source">
<pre>result = integer &gt;= 0
</pre></div>
<p>Although there is no defined minimum value for this result, as it is used to partition work during job submission, a block size that is too small will result in either too many jobs being submitted for efficient work, or the <tt>JobSubmissionClient</tt> running out of memory.</p>
<p>Any FileSystem that does not actually break files into blocks SHOULD return a number for this that results in efficient processing. A FileSystem MAY make this user-configurable (the S3 and Swift filesystem clients do this).</p></div></div>
<div class="section">
<h3><tt>getDefaultBlockSize(Path P)</tt><a name="getDefaultBlockSizePath_P"></a></h3>
<div class="section">
<h4>Preconditions<a name="Preconditions"></a></h4></div>
<div class="section">
<h4>Postconditions<a name="Postconditions"></a></h4>

<div class="source">
<pre>result = integer  &gt;= 0
</pre></div>
<p>The outcome of this operation is usually identical to <tt>getDefaultBlockSize()</tt>, with no checks for the existence of the given path.</p>
<p>Filesystems that support mount points may have different default values for different paths, in which case the specific default value for the destination path SHOULD be returned.</p></div></div>
<div class="section">
<h3><tt>getBlockSize(Path P)</tt><a name="getBlockSizePath_P"></a></h3>
<div class="section">
<h4>Preconditions<a name="Preconditions"></a></h4>

<div class="source">
<pre>if not exists(FS, p) :  raise FileNotFoundException
</pre></div></div>
<div class="section">
<h4>Postconditions<a name="Postconditions"></a></h4>

<div class="source">
<pre>result == getFileStatus(P).getBlockSize()
</pre></div>
<p>The outcome of this operation MUST be identical to that contained in the <tt>FileStatus</tt> returned from <tt>getFileStatus(P)</tt>.</p></div></div></div>
<div class="section">
<h2>State Changing Operations<a name="State_Changing_Operations"></a></h2>
<div class="section">
<h3><tt>boolean mkdirs(Path p, FsPermission permission )</tt><a name="boolean_mkdirsPath_p_FsPermission_permission_"></a></h3>
<p>Create a directory and all its parents</p>
<div class="section">
<h4>Preconditions<a name="Preconditions"></a></h4>

<div class="source">
<pre> if exists(FS, p) and not isDir(FS, p) :
     raise [ParentNotDirectoryException, FileAlreadyExistsException, IOException]
</pre></div></div>
<div class="section">
<h4>Postconditions<a name="Postconditions"></a></h4>

<div class="source">
<pre>FS' where FS'.Directories' = FS.Directories + [p] + ancestors(FS, p)
result = True
</pre></div>
<p>The condition exclusivity requirement of a FileSystem&#x2019;s directories, files and symbolic links must hold.</p>
<p>The probe for the existence and type of a path and directory creation MUST be atomic. The combined operation, including <tt>mkdirs(parent(F))</tt> MAY be atomic.</p>
<p>The return value is always true&#x2014;even if a new directory is not created  (this is defined in HDFS).</p></div>
<div class="section">
<h4>Implementation Notes: Local FileSystem<a name="Implementation_Notes:_Local_FileSystem"></a></h4>
<p>The local FileSystem does not raise an exception if <tt>mkdirs(p)</tt> is invoked on a path that exists and is a file. Instead the operation returns false.</p>

<div class="source">
<pre>if isFile(FS, p):
   FS' = FS
   result = False
</pre></div></div></div>
<div class="section">
<h3><tt>FSDataOutputStream create(Path, ...)</tt><a name="FSDataOutputStream_createPath_..."></a></h3>

<div class="source">
<pre>FSDataOutputStream create(Path p,
      FsPermission permission,
      boolean overwrite,
      int bufferSize,
      short replication,
      long blockSize,
      Progressable progress) throws IOException;
</pre></div>
<div class="section">
<h4>Preconditions<a name="Preconditions"></a></h4>
<p>The file must not exist for a no-overwrite create:</p>

<div class="source">
<pre>if not overwrite and isFile(FS, p)  : raise FileAlreadyExistsException
</pre></div>
<p>Writing to or overwriting a directory must fail.</p>

<div class="source">
<pre>if isDir(FS, p) : raise {FileAlreadyExistsException, FileNotFoundException, IOException}
</pre></div>
<p>FileSystems may reject the request for other reasons, such as the FS being read-only (HDFS), the block size being below the minimum permitted (HDFS), the replication count being out of range (HDFS), quotas on namespace or filesystem being exceeded, reserved names, etc. All rejections SHOULD be <tt>IOException</tt> or a subclass thereof and MAY be a <tt>RuntimeException</tt> or subclass. For instance, HDFS may raise a <tt>InvalidPathException</tt>.</p></div>
<div class="section">
<h4>Postconditions<a name="Postconditions"></a></h4>

<div class="source">
<pre>FS' where :
   FS'.Files'[p] == []
   ancestors(p) is-subset-of FS'.Directories'

result = FSDataOutputStream
</pre></div>
<p>The updated (valid) FileSystem must contains all the parent directories of the path, as created by <tt>mkdirs(parent(p))</tt>.</p>
<p>The result is <tt>FSDataOutputStream</tt>, which through its operations may generate new filesystem states with updated values of <tt>FS.Files[p]</tt></p></div>
<div class="section">
<h4>Implementation Notes<a name="Implementation_Notes"></a></h4>

<ul>
  
<li>
<p>Some implementations split the create into a check for the file existing  from the  actual creation. This means the operation is NOT atomic: it is possible for  clients creating files with <tt>overwrite==true</tt> to fail if the file is created  by another client between the two tests.</p></li>
  
<li>
<p>S3N, Swift and potentially other Object Stores do not currently change the FS state until the output stream <tt>close()</tt> operation is completed. This MAY be a bug, as it allows &gt;1 client to create a file with <tt>overwrite==false</tt>,  and potentially confuse file/directory logic</p></li>
  
<li>
<p>The Local FileSystem raises a <tt>FileNotFoundException</tt> when trying to create a file over a directory, hence it is is listed as an exception that MAY be raised when this precondition fails.</p></li>
  
<li>
<p>Not covered: symlinks. The resolved path of the symlink is used as the final path argument to the <tt>create()</tt> operation</p></li>
</ul></div></div>
<div class="section">
<h3><tt>FSDataOutputStream append(Path p, int bufferSize, Progressable progress)</tt><a name="FSDataOutputStream_appendPath_p_int_bufferSize_Progressable_progress"></a></h3>
<p>Implementations MAY throw <tt>UnsupportedOperationException</tt>.</p>
<div class="section">
<h4>Preconditions<a name="Preconditions"></a></h4>

<div class="source">
<pre>if not exists(FS, p) : raise FileNotFoundException

if not isFile(FS, p) : raise [FileNotFoundException, IOException]
</pre></div></div>
<div class="section">
<h4>Postconditions<a name="Postconditions"></a></h4>

<div class="source">
<pre>FS
result = FSDataOutputStream
</pre></div>
<p>Return: <tt>FSDataOutputStream</tt>, which can update the entry <tt>FS.Files[p]</tt> by appending data to the existing list.</p></div></div>
<div class="section">
<h3><tt>FSDataInputStream open(Path f, int bufferSize)</tt><a name="FSDataInputStream_openPath_f_int_bufferSize"></a></h3>
<p>Implementations MAY throw <tt>UnsupportedOperationException</tt>.</p>
<div class="section">
<h4>Preconditions<a name="Preconditions"></a></h4>

<div class="source">
<pre>if not isFile(FS, p)) : raise [FileNotFoundException, IOException]
</pre></div>
<p>This is a critical precondition. Implementations of some FileSystems (e.g. Object stores) could shortcut one round trip by postponing their HTTP GET operation until the first <tt>read()</tt> on the returned <tt>FSDataInputStream</tt>. However, much client code does depend on the existence check being performed at the time of the <tt>open()</tt> operation. Implementations MUST check for the presence of the file at the time of creation. This does not imply that the file and its data is still at the time of the following <tt>read()</tt> or any successors.</p></div>
<div class="section">
<h4>Postconditions<a name="Postconditions"></a></h4>

<div class="source">
<pre>result = FSDataInputStream(0, FS.Files[p])
</pre></div>
<p>The result provides access to the byte array defined by <tt>FS.Files[p]</tt>; whether that access is to the contents at the time the <tt>open()</tt> operation was invoked, or whether and how it may pick up changes to that data in later states of FS is an implementation detail.</p>
<p>The result MUST be the same for local and remote callers of the operation.</p></div>
<div class="section">
<h4>HDFS implementation notes<a name="HDFS_implementation_notes"></a></h4>

<ol style="list-style-type: decimal">
  
<li>
<p>HDFS MAY throw <tt>UnresolvedPathException</tt> when attempting to traverse symbolic links</p></li>
  
<li>
<p>HDFS throws <tt>IOException(&quot;Cannot open filename &quot; + src)</tt> if the path exists in the metadata, but no copies of any its blocks can be located; -<tt>FileNotFoundException</tt> would seem more accurate and useful.</p></li>
</ol></div></div>
<div class="section">
<h3><tt>FileSystem.delete(Path P, boolean recursive)</tt><a name="FileSystem.deletePath_P_boolean_recursive"></a></h3>
<div class="section">
<h4>Preconditions<a name="Preconditions"></a></h4>
<p>A directory with children and recursive == false cannot be deleted</p>

<div class="source">
<pre>if isDir(FS, p) and not recursive and (children(FS, p) != {}) : raise IOException
</pre></div></div>
<div class="section">
<h4>Postconditions<a name="Postconditions"></a></h4>
<div class="section">
<h5>Nonexistent path<a name="Nonexistent_path"></a></h5>
<p>If the file does not exist the FS state does not change</p>

<div class="source">
<pre>if not exists(FS, p):
    FS' = FS
    result = False
</pre></div>
<p>The result SHOULD be <tt>False</tt>, indicating that no file was deleted.</p></div>
<div class="section">
<h5>Simple File<a name="Simple_File"></a></h5>
<p>A path referring to a file is removed, return value: <tt>True</tt></p>

<div class="source">
<pre>if isFile(FS, p) :
    FS' = (FS.Directories, FS.Files - [p], FS.Symlinks)
    result = True
</pre></div></div>
<div class="section">
<h5>Empty root directory<a name="Empty_root_directory"></a></h5>
<p>Deleting an empty root does not change the filesystem state and may return true or false.</p>

<div class="source">
<pre>if isDir(FS, p) and isRoot(p) and children(FS, p) == {} :
    FS ' = FS
    result = (undetermined)
</pre></div>
<p>There is no consistent return code from an attempt to delete the root directory.</p></div>
<div class="section">
<h5>Empty (non-root) directory<a name="Empty_non-root_directory"></a></h5>
<p>Deleting an empty directory that is not root will remove the path from the FS and return true.</p>

<div class="source">
<pre>if isDir(FS, p) and not isRoot(p) and children(FS, p) == {} :
    FS' = (FS.Directories - [p], FS.Files, FS.Symlinks)
    result = True
</pre></div></div>
<div class="section">
<h5>Recursive delete of root directory<a name="Recursive_delete_of_root_directory"></a></h5>
<p>Deleting a root path with children and <tt>recursive==True</tt>  can do one of two things.</p>
<p>The POSIX model assumes that if the user has the correct permissions to delete everything, they are free to do so (resulting in an empty filesystem).</p>

<div class="source">
<pre>if isDir(FS, p) and isRoot(p) and recursive :
    FS' = ({[&quot;/&quot;]}, {}, {}, {})
    result = True
</pre></div>
<p>In contrast, HDFS never permits the deletion of the root of a filesystem; the filesystem can be taken offline and reformatted if an empty filesystem is desired.</p>

<div class="source">
<pre>if isDir(FS, p) and isRoot(p) and recursive :
    FS' = FS
    result = False
</pre></div></div>
<div class="section">
<h5>Recursive delete of non-root directory<a name="Recursive_delete_of_non-root_directory"></a></h5>
<p>Deleting a non-root path with children <tt>recursive==true</tt> removes the path and all descendants</p>

<div class="source">
<pre>if isDir(FS, p) and not isRoot(p) and recursive :
    FS' where:
        not isDir(FS', p)
        and forall d in descendants(FS, p):
            not isDir(FS', d)
            not isFile(FS', d)
            not isSymlink(FS', d)
    result = True
</pre></div></div></div>
<div class="section">
<h4>Atomicity<a name="Atomicity"></a></h4>

<ul>
  
<li>
<p>Deleting a file MUST be an atomic action.</p></li>
  
<li>
<p>Deleting an empty directory MUST be an atomic action.</p></li>
  
<li>
<p>A recursive delete of a directory tree MUST be atomic.</p></li>
</ul></div>
<div class="section">
<h4>Implementation Notes<a name="Implementation_Notes"></a></h4>

<ul>
  
<li>S3N, Swift, FTP and potentially other non-traditional FileSystems implement <tt>delete()</tt> as recursive listing and file delete operation. This can break the expectations of client applications -and means that they cannot be used as drop-in replacements for HDFS.</li>
</ul>
<!-- ============================================================= -->
<!-- METHOD: rename() -->
<!-- ============================================================= --></div></div>
<div class="section">
<h3><tt>FileSystem.rename(Path src, Path d)</tt><a name="FileSystem.renamePath_src_Path_d"></a></h3>
<p>In terms of its specification, <tt>rename()</tt> is one of the most complex operations within a filesystem .</p>
<p>In terms of its implementation, it is the one with the most ambiguity regarding when to return false versus raising an exception.</p>
<p>Rename includes the calculation of the destination path. If the destination exists and is a directory, the final destination of the rename becomes the destination + the filename of the source path.</p>

<div class="source">
<pre>let dest = if (isDir(FS, src) and d != src) :
        d + [filename(src)]
    else :
        d
</pre></div>
<div class="section">
<h4>Preconditions<a name="Preconditions"></a></h4>
<p>All checks on the destination path MUST take place after the final <tt>dest</tt> path has been calculated.</p>
<p>Source <tt>src</tt> must exist:</p>

<div class="source">
<pre>exists(FS, src) else raise FileNotFoundException
</pre></div>
<p><tt>dest</tt> cannot be a descendant of <tt>src</tt>:</p>

<div class="source">
<pre>if isDescendant(FS, src, dest) : raise IOException
</pre></div>
<p>This implicitly covers the special case of <tt>isRoot(FS, src)</tt>.</p>
<p><tt>dest</tt> must be root, or have a parent that exists:</p>

<div class="source">
<pre>isRoot(FS, dest) or exists(FS, parent(dest)) else raise IOException
</pre></div>
<p>The parent path of a destination must not be a file:</p>

<div class="source">
<pre>if isFile(FS, parent(dest)) : raise IOException
</pre></div>
<p>This implicitly covers all the ancestors of the parent.</p>
<p>There must not be an existing file at the end of the destination path:</p>

<div class="source">
<pre>if isFile(FS, dest) : raise FileAlreadyExistsException, IOException
</pre></div></div>
<div class="section">
<h4>Postconditions<a name="Postconditions"></a></h4>
<div class="section">
<h5>Renaming a directory onto itself<a name="Renaming_a_directory_onto_itself"></a></h5>
<p>Renaming a directory onto itself is no-op; return value is not specified.</p>
<p>In POSIX the result is <tt>False</tt>; in HDFS the result is <tt>True</tt>.</p>

<div class="source">
<pre>if isDir(FS, src) and src == dest :
    FS' = FS
    result = (undefined)
</pre></div></div>
<div class="section">
<h5>Renaming a file to self<a name="Renaming_a_file_to_self"></a></h5>
<p>Renaming a file to itself is a no-op; the result is <tt>True</tt>.</p>

<div class="source">
<pre> if isFile(FS, src) and src == dest :
     FS' = FS
     result = True
</pre></div></div>
<div class="section">
<h5>Renaming a file onto a nonexistent path<a name="Renaming_a_file_onto_a_nonexistent_path"></a></h5>
<p>Renaming a file where the destination is a directory moves the file as a child  of the destination directory, retaining the filename element of the source path.</p>

<div class="source">
<pre>if isFile(FS, src) and src != dest:
    FS' where:
        not exists(FS', src)
        and exists(FS', dest)
        and data(FS', dest) == data (FS, dest)
    result = True
</pre></div></div>
<div class="section">
<h5>Renaming a directory onto a directory<a name="Renaming_a_directory_onto_a_directory"></a></h5>
<p>If <tt>src</tt> is a directory then all its children will then exist under <tt>dest</tt>, while the path <tt>src</tt> and its descendants will no longer not exist. The names of the paths under <tt>dest</tt> will match those under <tt>src</tt>, as will the contents:</p>

<div class="source">
<pre>if isDir(FS, src) isDir(FS, dest) and src != dest :
    FS' where:
        not exists(FS', src)
        and dest in FS'.Directories]
        and forall c in descendants(FS, src) :
            not exists(FS', c))
        and forall c in descendants(FS, src) where isDir(FS, c):
            isDir(FS', dest + childElements(src, c)
        and forall c in descendants(FS, src) where not isDir(FS, c):
                data(FS', dest + childElements(s, c)) == data(FS, c)
    result = True
</pre></div></div>
<div class="section">
<h5>Renaming into a path where the parent path does not exist<a name="Renaming_into_a_path_where_the_parent_path_does_not_exist"></a></h5>

<div class="source">
<pre>  not exists(FS, parent(dest))
</pre></div>
<p>There is no consistent behavior here.</p>
<p><i>HDFS</i></p>
<p>The outcome is no change to FileSystem state, with a return value of false.</p>

<div class="source">
<pre>FS' = FS; result = False
</pre></div>
<p><i>Local Filesystem, S3N</i></p>
<p>The outcome is as a normal rename, with the additional (implicit) feature that the parent directores of the destination also exist</p>

<div class="source">
<pre>exists(FS', parent(dest))
</pre></div>
<p><i>Other Filesystems (including Swift) </i></p>
<p>Other filesystems strictly reject the operation, raising a <tt>FileNotFoundException</tt></p></div>
<div class="section">
<h5>Concurrency requirements<a name="Concurrency_requirements"></a></h5>

<ul>
  
<li>
<p>The core operation of <tt>rename()</tt>&#x2014;moving one entry in the filesystem to another&#x2014;MUST be atomic. Some applications rely on this as a way to coordinate access to data.</p></li>
  
<li>
<p>Some FileSystem implementations perform checks on the destination FileSystem before and after the rename. One example of this is <tt>ChecksumFileSystem</tt>, which provides checksummed access to local data. The entire sequence MAY NOT be atomic.</p></li>
</ul></div>
<div class="section">
<h5>Implementation Notes<a name="Implementation_Notes"></a></h5>
<p><b>Files open for reading, writing or appending</b></p>
<p>The behavior of <tt>rename()</tt> on an open file is unspecified: whether it is allowed, what happens to later attempts to read from or write to the open stream</p>
<p><b>Renaming a directory onto itself</b></p>
<p>The return code of renaming a directory onto itself is unspecified.</p>
<p><b>Destination exists and is a file</b></p>
<p>Renaming a file atop an existing file is specified as failing, raising an exception.</p>

<ul>
  
<li>
<p>Local FileSystem : the rename succeeds; the destination file is replaced by the source file.</p></li>
  
<li>
<p>HDFS : The rename fails, no exception is raised. Instead the method call simply returns false.</p></li>
</ul>
<p><b>Missing source file</b></p>
<p>If the source file <tt>src</tt> does not exist, <tt>FileNotFoundException</tt> should be raised.</p>
<p>HDFS fails without raising an exception; <tt>rename()</tt> merely returns false.</p>

<div class="source">
<pre>FS' = FS
result = false
</pre></div>
<p>The behavior of HDFS here should not be considered a feature to replicate. <tt>FileContext</tt> explicitly changed the behavior to raise an exception, and the retrofitting of that action to the <tt>DFSFileSystem</tt> implementation is an ongoing matter for debate.</p></div></div></div>
<div class="section">
<h3><tt>concat(Path p, Path sources[])</tt><a name="concatPath_p_Path_sources"></a></h3>
<p>Joins multiple blocks together to create a single file. This is a little-used operation currently implemented only by HDFS.</p>
<p>Implementations MAY throw <tt>UnsupportedOperationException</tt></p>
<div class="section">
<h4>Preconditions<a name="Preconditions"></a></h4>

<div class="source">
<pre>if not exists(FS, p) : raise FileNotFoundException

if sources==[] : raise IllegalArgumentException
</pre></div>
<p>All sources MUST be in the same directory:</p>

<div class="source">
<pre>for s in sources: if parent(S) != parent(p) raise IllegalArgumentException
</pre></div>
<p>All block sizes must match that of the target:</p>

<div class="source">
<pre>for s in sources: getBlockSize(FS, S) == getBlockSize(FS, p)
</pre></div>
<p>No duplicate paths:</p>

<div class="source">
<pre>not (exists p1, p2 in (sources + [p]) where p1 == p2)
</pre></div>
<p>HDFS: All source files except the final one MUST be a complete block:</p>

<div class="source">
<pre>for s in (sources[0:length(sources)-1] + [p]):
  (length(FS, s) mod getBlockSize(FS, p)) == 0
</pre></div></div>
<div class="section">
<h4>Postconditions<a name="Postconditions"></a></h4>

<div class="source">
<pre>FS' where:
 (data(FS', T) = data(FS, T) + data(FS, sources[0]) + ... + data(FS, srcs[length(srcs)-1]))
 and for s in srcs: not exists(FS', S)
</pre></div>
<p>HDFS&#x2019;s restrictions may be an implementation detail of how it implements <tt>concat</tt> -by changing the inode references to join them together in a sequence. As no other filesystem in the Hadoop core codebase implements this method, there is no way to distinguish implementation detail. from specification.</p></div></div></div>
      </div>
    </div>
    <div class="clear">
      <hr/>
    </div>
    <div id="footer">
      <div class="xright">&#169;            2015
              Apache Software Foundation
            
                       - <a href="http://maven.apache.org/privacy-policy.html">Privacy Policy</a></div>
      <div class="clear">
        <hr/>
      </div>
    </div>
  </body>
</html>
